{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb452123f16c92b6d0b9d2199b29f7a4d9f8bea6"
   },
   "source": [
    "# Alexnet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8d38b05f4ce087c69bcae46be6155b62a7792105"
   },
   "source": [
    "AlexNet is the name of a convolutional neural network, invented by **Alex Krizhevsky**, **Ilya Sutskever** and **Geoffrey Hinton**. AlexNet has had a large impact on the field of machine learning, specifically in the application of deep learning to machine vision. As of 2018 it has been cited over 25,000 times.\n",
    "\n",
    "AlexNet competed in the [ImageNet Large Scale Visual Recognition Challenge](https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge) in 2012. The network achieved a top-5 error of 15.3%, more than 10.8 percentage points lower than that of the runner up. The original paper's primary result was that the depth of the model was essential for its high performance, which was computationally expensive, but made feasible due to the utilization of GPUs during training.\n",
    "\n",
    "AlexNet contained eight layers; the first five were convolutional layers, and the last three were fully connected layers. It used the non-saturating `ReLU` activation function, which showed improved training performance over `tanh` and `sigmoid`.\n",
    "\n",
    "![](https://github.com/Koderunners/Convolutional-Neural-Networks/blob/master/Images/Alexnet.png)\n",
    "\n",
    "\n",
    "The current implementation has been made using Tensorflow which utilizes 6 GB of Nvidia Tesla K80 GPU provided in Kaggle Kernels. This kernel was written by [Soumik Rakshit](https://www.kaggle.com/soumikrakshit) and [Sohom Dey](https://www.kaggle.com/sohom17d)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8cf82a475cadeadc2687bc53bef02627bf782fec"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ganesha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import cv2, os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e24edfd1f23c8b94ba0bac7f48dee517184b032"
   },
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "c0b6185d01e91c3967db1eca2b18ffef2117fe38",
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'asl_alphabet_train/asl_alphabet_train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masl_alphabet_train/asl_alphabet_train/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m train_folders \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m test_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124masl_alphabet_test/asl_alphabet_test/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m test_files \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(test_dir)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'asl_alphabet_train/asl_alphabet_train/'"
     ]
    }
   ],
   "source": [
    "train_dir = 'asl_alphabet_train/asl_alphabet_train/'\n",
    "train_folders = os.listdir(train_dir)\n",
    "test_dir = 'asl_alphabet_test/asl_alphabet_test/'\n",
    "test_files = os.listdir(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e896cf03a073b67e29ed0ea9d1ad3303a89378e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = [], []\n",
    "for folder in train_folders:\n",
    "    files = os.listdir(train_dir + folder)\n",
    "    print('Reading images from ' + train_dir + folder + '/ ...')\n",
    "    for file in files[:1000]:\n",
    "        img = cv2.imread(train_dir + folder + '/' + file)\n",
    "        img = cv2.resize(img, (227, 227))\n",
    "        x_train.append(img)\n",
    "        y_train.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "450c435e4ebac23841149248f6d742f1e08f7d1d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(x_train), x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6808ce35b9ab623483c2fe9a2f49a6a496902b18",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21286e7cde9891eac40c5a32dcc869f11d1d741f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_test, y_test = [], []\n",
    "for file in test_files:\n",
    "    img = cv2.imread(test_dir + file)\n",
    "    img = cv2.resize(img, (227, 227))\n",
    "    x_test.append(img)\n",
    "    y_test.append(file.split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62229833596941615fc68f85885bef315a2f2c88",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(x_test), x_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "414e3b19ea9e4e9cbfa4c67c2a181875deeb2fcb",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "89a26fb1d4d8a676bb053fe25dc4fa969ed68c18"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8fc4d352eafba9ebd7a2ed2005ba828d7dbc1518",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LabelEncoding\n",
    "y_test_encoded = np.array(list(range(len(y_test))))\n",
    "y_train_encoded = np.array([y_test.index(i) if i != 'del' else 29 for i in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10b414da8bf2a891f525589d7d3fdd0924cc0a80",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_test_encoded = np.eye(30)[y_test_encoded]\n",
    "y_train_encoded = np.eye(30)[y_train_encoded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3066ecd34d8e25d1dbf90d8139b15ae591094dd8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train_encoded.shape, y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "418887b5748020f100f68d2ace1f0fd6e63436ca"
   },
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c25a0b24ad933aea445c0d53eb927b8126aceba1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def next_batch(batch_size, data, labels):\n",
    "    idx = np.arange(0, len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[: batch_size]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a373ba21959ec689e4beef98513d0eb10265d9dd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def display_images(data, title, display_label = True):\n",
    "    x, y = data\n",
    "    fig, axes = plt.subplots(2, 6, figsize = (18, 5))\n",
    "    fig.subplots_adjust(hspace = 0.5, wspace = 0.5)\n",
    "    fig.suptitle(title, fontsize = 18)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(x[i])\n",
    "        if display_label:\n",
    "            ax.set_xlabel(y[i])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "643112e977e0a47bb36fbba360a79b06c31bbfcd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display_images(next_batch(12, x_train, y_train), 'Training Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3652cd3643c149f5165d60bb38a359dbc51bf685",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display_images(next_batch(12, x_test, y_test), 'Test Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a5c95e2d52ee4d33eaa5fe4413a58902d02ef4fa"
   },
   "source": [
    "## Alexnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ec8ab981a381a3d65bde300ed305fd12505b185f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "learning_rate = 0.001\n",
    "epochs = 5000\n",
    "batch_size = 128\n",
    "display_step = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b67a042032ea5ae5eec536debff2a2003bbd273",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "height = 227\n",
    "width = 227\n",
    "n_channels = 3\n",
    "n_classes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "119e555c37888f7b85b7cc15ce50a04b03d4a888",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = [None, height, width, n_channels])\n",
    "Y = tf.placeholder(tf.float32, shape = [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "21a597727137f7d9d23a097d44ba289d06259960",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # Convolutional Layer 1: 11x11 filters, 3 input channels, 96 output channels\n",
    "    'w1' : tf.Variable(tf.random_normal([11, 11, 3, 96])), \n",
    "    # Convolutional Layer 2: 5x5 filters, 96 input channels, 256 output channels\n",
    "    'w2' : tf.Variable(tf.random_normal([5, 5, 96, 256])),\n",
    "    # Convolutional Layer 3: 3x3 filters, 256 input channels, 384 output channels\n",
    "    'w3' : tf.Variable(tf.random_normal([3, 3, 256, 384])),\n",
    "    # Convolutional Layer 4: 3x3 filters, 384 input channels, 384 output channels\n",
    "    'w4' : tf.Variable(tf.random_normal([3, 3, 384, 384])),\n",
    "    # Convolutional Layer 5: 3x3 filters, 384 input channels, 256 output channels\n",
    "    'w5' : tf.Variable(tf.random_normal([3, 3, 384, 256])),\n",
    "    # Fully Connected Layer 1: 9216 input channels, 4096 output channels\n",
    "    'w6' : tf.Variable(tf.random_normal([9216, 4096])),\n",
    "    # Fully Connected Layer 2: 4096 input channels, 4096 output channels\n",
    "    'w7' : tf.Variable(tf.random_normal([4096, 4096])),\n",
    "    # Fully Connected Layer 3: 4096 input channels, 30(number of classes) output channels\n",
    "    'w8' : tf.Variable(tf.random_normal([4096, n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0e2b840360aab6f3deb9d1002bb2a9d9f52799c0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "biases = {\n",
    "    'b1' : tf.Variable(tf.random_normal([96])),\n",
    "    'b2' : tf.Variable(tf.random_normal([256])),\n",
    "    'b3' : tf.Variable(tf.random_normal([384])),\n",
    "    'b4' : tf.Variable(tf.random_normal([384])),\n",
    "    'b5' : tf.Variable(tf.random_normal([256])),\n",
    "    'b6' : tf.Variable(tf.random_normal([4096])),\n",
    "    'b7' : tf.Variable(tf.random_normal([4096])),\n",
    "    'b8' : tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3f85b996d4859ddd23eac33f5fe3dbd266b2efef",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Wrapper function for creating a Convolutional Layer\n",
    "def conv2d(x, W, b, strides = 1, padding = 'SAME'):\n",
    "    x = tf.nn.conv2d(x, W, strides = [1, strides, strides, 1], padding = padding)\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8bf489e488ca3d9e5acefe82a5d54dcb1ddff8c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Wrapper function for creating a Pooling Layer\n",
    "def maxpool2d(x, k = 2, padding = 'VALID'):\n",
    "    return tf.nn.max_pool(x, ksize = [1, k, k, 1], strides = [1, k, k, 1], padding = padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8a89a355f1cd74582f135b387cd9c71966bf38e6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def alexnet(x, w, b):\n",
    "    x = tf.reshape(x, shape = [-1, 227, 227, 3])\n",
    "    \n",
    "    # Layer 1\n",
    "    conv1 = conv2d(x, w['w1'], b['b1'], strides = 4, padding = 'VALID') # Convolution\n",
    "    conv1 = maxpool2d(conv1) # Pooling\n",
    "    \n",
    "    # Layer 2\n",
    "    conv2 = conv2d(conv1, w['w2'], b['b2']) # Convolution\n",
    "    conv2 = maxpool2d(conv2) # Pooling\n",
    "    \n",
    "    # Layer 3\n",
    "    conv3 = conv2d(conv2, w['w3'], b['b3']) # Convolution\n",
    "    \n",
    "    # Layer 4\n",
    "    conv4 = conv2d(conv3, w['w4'], b['b4']) # Convolution\n",
    "    \n",
    "    # Layer 5\n",
    "    conv5 = conv2d(conv4, w['w5'], b['b5']) # Convolution\n",
    "    conv5 = maxpool2d(conv5) # Pooling\n",
    "    \n",
    "    # Layer 6\n",
    "    fc1 = tf.reshape(conv5, [-1, weights['w6'].get_shape().as_list()[0]]) # Channel Reshape\n",
    "    fc1 = tf.add(tf.matmul(fc1, w['w6']), b['b6']) # Linear Function\n",
    "    fc1 = tf.nn.relu(fc1) # Activation Function\n",
    "    \n",
    "    # Layer 7\n",
    "    fc2 = tf.add(tf.matmul(fc1, w['w7']), b['b7']) # Linear Function\n",
    "    fc2 = tf.nn.relu(fc2) # Activation Function\n",
    "    \n",
    "    # Layer 8\n",
    "    out = tf.add(tf.matmul(fc2, w['w8']), b['b8']) # Linear Function\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b278f7408c1186399c8c12212e38967070f498ac",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logits = alexnet(X, weights, biases) # Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd7c573ac7acd9a7c6db59ea319e6f4318df0f05",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Cost Function\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = logits, labels = Y))\n",
    "# Optimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "# Training Operation\n",
    "train_op = optimizer.minimize(loss_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6f2c43b5e19d15e886864f7dfd8c139aa63f98d8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5184d67f9e547d5846fdd25f6fd12bfc229bacd",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "311d763affa0f731d378f78a04b0f6e9a9cd5ee5"
   },
   "source": [
    "## Training the Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "63829ce38c0d7d785791c6cc842242aef4d1aa02",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    # Running Initializer\n",
    "    sess.run(init)\n",
    "    cost_hist, acc_hist = [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        _x, _y = next_batch(batch_size, x_train, y_train_encoded)\n",
    "        # Running Optimizer\n",
    "        sess.run(train_op, feed_dict = { X : _x, Y : _y })\n",
    "        if epoch % display_step == 0:\n",
    "            # Calculating Loss and Accuracy on the current Epoch\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict = { X : _x, Y : _y })\n",
    "            loss = loss\n",
    "            cost_hist.append(loss)\n",
    "            acc_hist.append(acc)\n",
    "            print('Epoch ' + str(epoch) + ', Cost: ' + str(loss) + ', Accuracy: ' + str(acc * 100) + ' %')\n",
    "    W = sess.run(weights)\n",
    "    B = sess.run(biases)\n",
    "    print('-' * 70)\n",
    "    print('\\nOptimization Finished\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f974ea7d9d97722522dc2a2d45247aa8ec35a538",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(cost_hist))), cost_hist)\n",
    "plt.title(\"Change in cost\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "35031d3d6468191a73124f81f567f20e99cac81a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(acc_hist))), acc_hist)\n",
    "plt.title(\"Change in accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a60503fda1853f9bc16464dddc5c99bdd6cda8a"
   },
   "source": [
    "## Saving Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6430bb072e0ac48e64a674bf7f5bee1715ac683",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for key in weights.keys():\n",
    "    weights[key] = tf.Variable(W[key])\n",
    "    np.save(key, W[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b76d097c323f30c71919d08cb99dca9abcf1c9a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for key in biases.keys():\n",
    "    biases[key] = tf.Variable(B[key])\n",
    "    np.save(key, B[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e23b335f33fc3cd8fbca4af00f7037c9f8861a2"
   },
   "source": [
    "## Checking Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58f30a54ae4a272ce19c816feeae2f50dc945f57",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "logits = alexnet(X, weights, biases)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ae06a15bff403f6a68a02fa8b2ad3f32d0caeab",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    acc = []\n",
    "    sess.run(init)\n",
    "    for i in range(100, 29001, 100):\n",
    "        acc.append(sess.run(accuracy, feed_dict = { X : x_train[i - 100 : i], Y : y_train_encoded[i - 100 : i] }))\n",
    "print('Accuracy on Training Data: ' + str(sum(acc) * 100 / len(acc)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fa4e27160b7627c5ea6e7e9acc6d07dc4445d393",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    y_pred = sess.run(logits, feed_dict = { X : x_test })\n",
    "    acc = sess.run(accuracy, feed_dict = { X : x_test, Y : y_test_encoded }) * 100\n",
    "print('Accuracy on Test Data: ' + str(acc) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1e916298081c536084bd270a072c6930b7984c3"
   },
   "source": [
    "## Visualizing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "412df66b4e84944b412530f34430ac9d06d25236",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_pred = [y_test[list(i).index(max(list(i)))] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76e173e163cd133afdd67d254c5aec63cfe32d92",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "display_images(next_batch(12, x_test, y_pred), 'Predictions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
